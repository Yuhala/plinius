#include "gru_layer.h"
#include "connected_layer.h"
#include "utils.h"
//#include "cuda.h"
#include "blas.h"
#include "gemm.h"

#include <math.h>
#include "dnet_sgx_utils.h"

static void increment_layer(layer *l, int steps)
{
    int num = l->outputs * l->batch * steps;
    l->output += num;
    l->delta += num;
    l->x += num;
    l->x_norm += num;
}

layer make_gru_layer(int batch, int inputs, int outputs, int steps, int batch_normalize, int adam)
{
#ifdef DNET_SGX_DEBUG
    printf("GRU Layer: %d inputs, %d outputs\n", inputs, outputs);
#endif
    batch = batch / steps;
    layer l = {0};
    l.batch = batch;
    l.type = GRU;
    l.steps = steps;
    l.inputs = inputs;

    l.uz = malloc(sizeof(layer));
#ifdef DNET_SGX_DEBUG
    printf("\t\t");
#endif
    *(l.uz) = make_connected_layer(batch * steps, inputs, outputs, LINEAR, batch_normalize, adam);
    l.uz->batch = batch;

    l.wz = malloc(sizeof(layer));
#ifdef DNET_SGX_DEBUG
    printf("\t\t");
#endif
    *(l.wz) = make_connected_layer(batch * steps, outputs, outputs, LINEAR, batch_normalize, adam);
    l.wz->batch = batch;

    l.ur = malloc(sizeof(layer));
#ifdef DNET_SGX_DEBUG
    printf("\t\t");
#endif
    *(l.ur) = make_connected_layer(batch * steps, inputs, outputs, LINEAR, batch_normalize, adam);
    l.ur->batch = batch;

    l.wr = malloc(sizeof(layer));
#ifdef DNET_SGX_DEBUG
    printf("\t\t");
#endif
    *(l.wr) = make_connected_layer(batch * steps, outputs, outputs, LINEAR, batch_normalize, adam);
    l.wr->batch = batch;

    l.uh = malloc(sizeof(layer));
#ifdef DNET_SGX_DEBUG
    printf("\t\t");
#endif
    *(l.uh) = make_connected_layer(batch * steps, inputs, outputs, LINEAR, batch_normalize, adam);
    l.uh->batch = batch;

    l.wh = malloc(sizeof(layer));
#ifdef DNET_SGX_DEBUG
    printf("\t\t");
#endif
    *(l.wh) = make_connected_layer(batch * steps, outputs, outputs, LINEAR, batch_normalize, adam);
    l.wh->batch = batch;

    l.batch_normalize = batch_normalize;

    l.outputs = outputs;
    l.output = calloc(outputs * batch * steps, sizeof(float));
    l.delta = calloc(outputs * batch * steps, sizeof(float));
    l.state = calloc(outputs * batch, sizeof(float));
    l.prev_state = calloc(outputs * batch, sizeof(float));
    l.forgot_state = calloc(outputs * batch, sizeof(float));
    l.forgot_delta = calloc(outputs * batch, sizeof(float));

    l.r_cpu = calloc(outputs * batch, sizeof(float));
    l.z_cpu = calloc(outputs * batch, sizeof(float));
    l.h_cpu = calloc(outputs * batch, sizeof(float));

    l.forward = forward_gru_layer;
    l.backward = backward_gru_layer;
    l.update = update_gru_layer;

    return l;
}

void update_gru_layer(layer l, update_args a)
{
    update_connected_layer(*(l.ur), a);
    update_connected_layer(*(l.uz), a);
    update_connected_layer(*(l.uh), a);
    update_connected_layer(*(l.wr), a);
    update_connected_layer(*(l.wz), a);
    update_connected_layer(*(l.wh), a);
}

void forward_gru_layer(layer l, network net)
{
    network s = net;
    s.train = net.train;
    int i;
    layer uz = *(l.uz);
    layer ur = *(l.ur);
    layer uh = *(l.uh);

    layer wz = *(l.wz);
    layer wr = *(l.wr);
    layer wh = *(l.wh);

    fill_cpu(l.outputs * l.batch * l.steps, 0, uz.delta, 1);
    fill_cpu(l.outputs * l.batch * l.steps, 0, ur.delta, 1);
    fill_cpu(l.outputs * l.batch * l.steps, 0, uh.delta, 1);

    fill_cpu(l.outputs * l.batch * l.steps, 0, wz.delta, 1);
    fill_cpu(l.outputs * l.batch * l.steps, 0, wr.delta, 1);
    fill_cpu(l.outputs * l.batch * l.steps, 0, wh.delta, 1);
    if (net.train)
    {
        fill_cpu(l.outputs * l.batch * l.steps, 0, l.delta, 1);
        copy_cpu(l.outputs * l.batch, l.state, 1, l.prev_state, 1);
    }

    for (i = 0; i < l.steps; ++i)
    {
        s.input = l.state;
        forward_connected_layer(wz, s);
        forward_connected_layer(wr, s);

        s.input = net.input;
        forward_connected_layer(uz, s);
        forward_connected_layer(ur, s);
        forward_connected_layer(uh, s);

        copy_cpu(l.outputs * l.batch, uz.output, 1, l.z_cpu, 1);
        axpy_cpu(l.outputs * l.batch, 1, wz.output, 1, l.z_cpu, 1);

        copy_cpu(l.outputs * l.batch, ur.output, 1, l.r_cpu, 1);
        axpy_cpu(l.outputs * l.batch, 1, wr.output, 1, l.r_cpu, 1);

        activate_array(l.z_cpu, l.outputs * l.batch, LOGISTIC);
        activate_array(l.r_cpu, l.outputs * l.batch, LOGISTIC);

        copy_cpu(l.outputs * l.batch, l.state, 1, l.forgot_state, 1);
        mul_cpu(l.outputs * l.batch, l.r_cpu, 1, l.forgot_state, 1);

        s.input = l.forgot_state;
        forward_connected_layer(wh, s);

        copy_cpu(l.outputs * l.batch, uh.output, 1, l.h_cpu, 1);
        axpy_cpu(l.outputs * l.batch, 1, wh.output, 1, l.h_cpu, 1);

        if (l.tanh)
        {
            activate_array(l.h_cpu, l.outputs * l.batch, TANH);
        }
        else
        {
            activate_array(l.h_cpu, l.outputs * l.batch, LOGISTIC);
        }

        weighted_sum_cpu(l.state, l.h_cpu, l.z_cpu, l.outputs * l.batch, l.output);

        copy_cpu(l.outputs * l.batch, l.output, 1, l.state, 1);

        net.input += l.inputs * l.batch;
        l.output += l.outputs * l.batch;
        increment_layer(&uz, 1);
        increment_layer(&ur, 1);
        increment_layer(&uh, 1);

        increment_layer(&wz, 1);
        increment_layer(&wr, 1);
        increment_layer(&wh, 1);
    }
}

void backward_gru_layer(layer l, network net)
{
}
